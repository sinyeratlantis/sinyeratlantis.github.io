## 通用智能本质的总结

以下就是我所理解的智能的本质。

智能的核心要素在于：观测、决策、预测、欲望。

### 1. 观测与决策

观测与决策是构建智能的第一步，其要求一个智能体必须要拥有可观测的环境、可执行的动作、以及其动作改变其观测的能力。所有智能相关的任务，首先需要解决的就是观测与决策问题。

据此，图像分类任务可以说不是一个智能任务，而是函数拟合任务，因为其动作（输出类别）不能干涉环境。实际上深度学习业界与通用智能最不符合的一点就是，很少任务涉及时序上的决策，而这对智能至关重要。

「观测」与「决策」更多是一项工程问题，虽然处理思路很明确，但至关重要。可观测的环境、可执行的动作、以及其动作改变其观测的能力，这些是构建一个智能体最基本的需求。

### 2. 预测

「预测」的核心在于联系与记忆，其将共现（时序关联）的刺激作为模式存储下来，并在「观测」的刺激下搜索与输入相似的模式，然后依据其时空关联（时间刺激相关性与空间刺激相关性），推导出相关内容，完成预测。预测模块包含了世界的常用模型，这个模型的本质是世间万物的时空关联性。

例如，我们到一个全新的场景总会不知所措，这是因为预测模块没有记忆信息，无法给出其所能预测的状态空间集，欲望模块就不知道该执行什么决策获得更好的状态，导致年轻人行为的随机选择。对有阅历的长者而言，预测模块中随意行为与负面结果已经相联系了，故会恐惧，进而采取保守的行为。

### 3. 欲望

人本能性地对状态就有倾向性，对某些状态赋予倾向（如温暖、饥饿、群居、性爱等），这些状态作为期望状态，在预测模块预测的状态空间集中赋予高值，欲望模块指导决策选择复现预测模块序列的方式，其具体方式如下：自身的行为总与环境观测的行为共现，则出现联系，预测模块预测得到模糊时序模式，激发联系扩散，通过记忆的行为联系刺激自身的行为，实现决策。

机器在上述过程中的问题主要在于机器无法对状态赋值。人对「观测」的处理是极为复杂有效的，能将大量观测刺激转化为高层响应并由基因编码对这些响应的基本赋值，实现欲望。欲望是根本性的，非学习性的，后期人类在学习过程中可能习得很多获得更愉悦的方式，但基本的舒适判断法早已确定。所以这部分是人类必须要设定给机器的。

设定机器欲望的方法目前有些尝试，如一般的强化学习任务设定机器目标为最大化某些奖励，自然这种设定方式毫无通用性可言。还有就是好奇心机制，使机器对于新的状态赋予高倾向值，这有可能作为我们起始的设计。

「欲望」很复杂，手工设定几乎必定陷入局部最优，要想让机器智能地为我们服务，就要让机器学习如何为其预测状态集赋值。比较可行的一种方法是，通过好奇心欲望使机器搜集到足够多而精确的模式记忆，然后直接输入自然语言，令自然语言对应的场景赋高值。如指令「让人类幸福」，其自行找到其对应的场景并赋高值。这样机器就拥有了让人类幸福的欲望。具体是否可行还是很大疑问，目前远未到此设计层面。

### 4. 落实到算法

观测为刺激输入，决策为刺激输出，预测为中间处理，欲望为状态判断。观测与决策在游戏AI里已经可以实现，转换到现实世界也不是太困难的事，那么，核心就在于预测与欲望了。欲望思路已经提及，剩下的就是预测了。

实现预测的核心在于要实现一个高度联系的世界模型。我们此前已经尝试过依据生物启发实现空间模式的压缩与记忆。不难发现，对于空变模式的联系而言，关于人脑的研究已经给了我们启发，人脑通过重复加强突触执行记忆、通过共现加强突触执行联系，其结果就是将多次出现的空间模式记忆了下来，而联系则包含在了模式中。所以看待图片需要以联系的观点看待，看成是点与点在空间上刺激的联系，很多疑问就会浮现解答。

所以，我们要做的就是将我们在空间模式上的思维方式扩展到时间模式上，使我们的模型能够建立起一个庞大而稳健的联系模型，成为我们日后决策的基础。这个模型，必须包含智能体对自身行动的理解，所以需要在决策环境中学习。

### 5. 强化学习在智能观点下的本质

强化学习解决的任务通常是序列决策问题，其拥有状态的观测、可执行的行为、以及决策影响观测的能力。也就是说对于智能而言前提已满足，接下来看它是如何决策的。

强化学习将状态与动作的关系直接建立为一个函数映射，这个函数是可学习的，以奖励信息为监督，通过优化方法拟合到使奖励值最大的映射函数即是成功了。虽然简单粗暴，但确实是部分解决了复杂序列决策的问题，而且一并解决了智能问题里的预测和欲望问题。当然，基于优化的算法有其共有的局限性，实现极度依赖于问题的可优化性（动作空间和状态空间需要非常有限）和奖励的设定（只能解决单一问题，没有扩展性），所以星际2的游戏AI实际上对于这种手段是很困难的。DeepMind的AlphaStar推测使用了宏动作和极度夸张的算力解决了可优化性的问题，使用监督数据的学习构造基本智能体解决了奖励稀疏的问题，将星际2强行转化为了一个靠算力即可突破的问题。

DeepMind的贡献无疑是巨大的，AlphaStar的出现意味着大部分序列决策问题，只要把目标设定好，状态动作空间可控，那么算力就可以完成剩下的事了，这对生产力的提升无疑是巨大的。而且，无疑强化学习领域将产生更多的岗位，因为将出现需要针对算法设计奖励和宏动作等工作了。当然，问题恐怕更加巨大，因为这种算法没有任何通用性可言，我们根本没法设想如何设计出精确的奖励，使智能体自我学习出一个足够通用的决策模型。工程上恐怕将有无数的应用可以与强化学习结合，如NLP与强化学习的结合将能使语言智能更加强大，但这些基本是与强人工智能无关的事情。

### 6. 展望

构造通用智能，入手点定然是在「观测」与「决策」完全的环境中测试和执行。星际2是一个很好的平台，动作空间清晰，执行清晰，观测清晰，剩下的就只是如何根据「观测」选取「决策」了。

现实意义上看，强化学习前景很好，作为立身之本不用担心。理想意义上看，强化学习是目前人工智能界与通用智能最接近的方法（虽然还是差很远），其余的除了类脑视觉能搭上预测的边，其他都不值一提。

所以，强化学习作为我们的立身之本是最好的，在此基础上探究我们关于通用智能的想法如何在决策过程中一步步落实是最佳的途径。强化学习无疑是我们研究通用智能最可行的研究方向。













