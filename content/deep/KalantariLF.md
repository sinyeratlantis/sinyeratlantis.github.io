## 基于学习的光场相机视图合成

Learning-Based View Synthesis for Light Field Cameras

Nima Khademi Kalantari - SIGGRAPH Asia 2016

### 摘要

随着消费者光场相机的引入，光场成像最近变得普遍。然而，在角度和空间分辨率之间存在固有的折衷，因此，这些相机通常在空间或空间域中稀疏地采样。在本文中，我们使用机器学习来减轻这种权衡。具体来说，我们提出了一种新的基于学习的方法，用于从稀疏的输入视图集合成新的视图。我们以现有的视图合成技术为基础，将过程分解为视差和颜色估计组件。我们使用两个顺序卷积神经网络来模拟这两个组件，并通过最小化合成和基本真实图像之间的误差来同时训练两个网络。我们仅使用Lytro Illum相机拍摄的光场中的四个角子孔径视图来显示我们的方法的性能。实验结果表明，我们的方法综合了高质量的图像，这些图像在各种具有挑战性的真实世界场景中都优于最先进的技术。我们相信我们的方法可能会降低消费者光场相机所需的角度分辨率，从而增加其空间分辨率。

### 1. 介绍

光场提供了丰富的真实世界场景表现，实现了重新聚焦和视点变化等激动人心的应用。 通常，它们是通过从不同视图捕获一组2D图像或使用微透镜阵列获得的。 早期的光场摄像机需要定制的摄像机设置，这些摄像机设置庞大且昂贵，因此不为公众所用。 最近，随着商用光场相机如Lytro和RayTrix的引入，人们对光场成像产生了新的兴趣。 然而，由于传感器的分辨率有限，角度和空间分辨率之间存在固有的折衷，这意味着光场相机在角度或空间域中稀疏地采样。 例如，Pelican相机有一组2×2相机。

为了缓解这个问题，我们提出了一种基于学习的方法，从使用消费者光场相机捕获的稀疏输入视图集合来合成新颖视图。 受到最近在各种应用中成功深度学习的启发，例如图像去噪、超分辨率和去模糊，我们建议使用卷积神经网络来使用稀疏输入视图和新视图的位置来预测光场新视图 。 然而，主要的挑战是为这项任务训练单个端到端CNN是困难的，产生相当模糊的新颖视图，如图6所示。

![](/Users/sinyeratlantis/Desktop/pic/4-6.png)

图6：我们将我们的方法与单个CNN架构进行比较。 我们分别在顶行和底行显示FLOWER 1和ROCK场景的插图。 一个CNN不能模拟输入图像和新颖视图图像之间的复杂关系，因此产生具有重影和其他伪像的结果。 相比之下，我们的系统包含两个连续的CNN，能够正确地建立关系模型，并产生高质量的结果，与实际情况相当。 可以在补充视频中找到所有合成视图的比较。

现有视图合成方法通常首先估计输入视图处的深度并使用它来将输入图像扭曲（插值）到新视图。然后，他们以特定方式组合这些图像（例如，通过对每个扭曲图像进行加权）以获得最终的新颖视图图像。为了使学习更容易处理，我们在这些方法的基础上，将任务分解为视差和颜色估计组件。我们工作的主要贡献是使用机器学习对这两个组件进行建模，并通过直接最小化合成和基本真实图像之间的误差来训练两个模型。在我们的系统中，我们使用两个连续的CNN来估计视差和最终的像素颜色。由于我们的视差估计CNN被训练以直接最小化合成误差，因此我们的估计的视差比现有的视差估计技术更适合于该应用（参见图10）。此外，由于我们在消费者光场相机产生的光场上训练我们的系统，因此它学会模拟这些相机的噪声和其他不准确性。因此，我们的方法比Wanner和Gold-2 luecke的最先进的基于优化的方法产生更好的结果，如图2所示。

![](/Users/sinyeratlantis/Desktop/pic/4-2.png)

图2：这个场景展示了一辆汽车（右侧）和一条街道上的海马雕像。 Wanner和Goldluecke的方法在输入视图处获取估计的视差并在新视图处合成图像。我们使用几种最先进的光场视差估计方法作为其输入，将我们的方法与该方法进行比较。我们仅使用Lytro Illum相机拍摄的光场的四个角落图像作为输入，并使用所有这些技术合成一个中间视图。尽管这些视差估计方法产生合理的视差图，但它们并非专门用于视图合成。此外，Wanner和Goldluecke的方法假设图像是理想的，而在实践中，用商用光场相机拍摄的图像通常包含噪声并遭受光学畸变。因此，这些方法会产生撕裂、重影和其他伪影的结果。然而，我们的方法学会处理这些不准确性并产生适合于该应用的视差。因此，我们可以生成高质量的图像，而不会在PSNR（dB）和结构相似性（SSIM）方面以视觉和数字方式干扰优于其他方法的伪像。较大的SSIM值显示出更好的感知质量。请注意，只有我们的方法能够分别重建绿色和蓝色插图中的高光和暗结构。参见图10，比较估计的视差。

我们仅使用Lytro Illum相机捕获的$8×8^2$的光场的四个角子孔径视图来演示我们的方法的性能（参见图1）。 实验结果表明，我们的方法在挑战性案例方面优于最先进的方案。 我们的方法比最近基于学习的Flynn的DeepStereo方法快两个数量级，从四个输入视图大小为541×376合成图像仅需12.3秒。我们的系统可能用于降低当前的相机所需的角分辨率，增加空间分辨率。 我们的方法的另一个应用是增加当前相机的基线，并在四个角度视图的子集上使用我们的方法来合成中间视图。 总之，我们做出以下贡献：

- 我们提出了第一种使用消费者光场相机进行视图合成的机器学习方法。 我们的系统由视差和颜色估计组件组成，我们使用两个连续的CNN进行建模。 注意，尽管CNN最近已用于光场超分辨率和深度估计，但这些方法不能直接在任意位置合成新视图。
- 我们的第一个网络的输出是不一致的，通常我们需要基本真实视差来训练这个网络。 然而，我们展示了如何通过直接最小化合成和基本真实图像之间的误差来同时训练两个网络。
- 由于我们以这种方式训练我们的视差估计器，我们的视差适合于视图合成应用。 据我们所知，我们的方法是第一个提出专门为此应用设计的视差估计器的方法。

![](/Users/sinyeratlantis/Desktop/pic/4-1.png)

图1：我们提出了一种基于学习的方法，用消费者光场摄像机捕获的一组稀疏输入视图合成新颖的视图。 我们使用Lytro Illum相机捕获角度分辨率为8×8的光场，并仅使用四个角落子孔径图像作为输入。 我们基于学习的方法能够处理花朵和背景之间的遮挡边界，并产生与基本真实图像相当的合理图像。 我们展示了与图7中最先进方法的比较。通过放大到纸张的电子版本，可以最好地看到整篇论文中的数字。

### 2. 相关工作

过去已经广泛研究了光场有限分辨率的问题，并且已经提出了几种用于增加角度和空间域中的分辨率的有效方法。 为简洁起见，我们只关注为角度超分辨率设计的方法。 我们首先回顾一下专门用于光场的算法，然后解释为一般场景和对象执行视图合成的方法。

#### 2.1 光场超分辨

Levin和Durand使用基于维度间隙的先验来重建来自3D焦点堆栈序列的完整4D光场。 Shi利用连续傅里叶谱中的稀疏性来重建来自1D组视点的密集光场。 Schedl使用来自稀疏输入视图集的多维补丁重建完整的光场。 这些方法要求以特定模式捕获输入样本，并且不能在任意位置合成新颖视图。 Marwah提出了一种基于字典的方法，用于从编码的2D投影重建光场。 然而，他们的方法需要以压缩方式捕获光场。

Mitra和Veeraraghavan引入了基于补丁的方法，他们使用高斯混合模型对光场斑块进行建模。然而，这种方法对于噪声并不健壮，并且在使用商业光场相机拍摄的低质量图像上会十分难以使用。Zhang提出了一种基于阶段的方法来重建光场。然而，他们的方法是有限的，因为它是为微基线立体对设计的。而且，他们的方法是迭代的，这通常是缓慢的并且阻止其在实践中的使用。 Yoon使用卷积神经网络对光场进行空间和角度超分辨率。但是，它们的方法只能将分辨率提高两倍，并且无法在任意位置合成视图。 Zhang已经提出了用于各种光场编辑应用的基于分层补片的合成。虽然它们对于填孔和重新洗孔等应用表现出令人印象深刻的结果，但它们的方法对于视图合成的性能有限，并且无法处理具有挑战性的情况，如图9所示。

最近，Wanner和Goldluecke提出了一种优化方法，用于在输入光场的新视图中重建图像。给定输入视图的深度估计，他们通过最小化最大化最终结果质量的目标函数来重建新颖的视图。尽管他们的方法在密集光场上产生合理的结果，但对于稀疏输入视图，它通常会产生撕裂、重影和其他伪影的结果，如图2所示。我们认为这是由于两个主要原因。首先，他们将输入视图中的视差估计为预处理，与视图合成过程无关。然而，即使是现有技术的光场视差估计技术通常也不旨在最大化合成视图的质量，因此它们不适合于该应用。其次，Wanner和Goldluecke的方法假定在理想条件下捕获图像。然而，在实践中，来自消费者光场相机的图像通常是有噪声的并且遭受光学畸变。

#### 2.2 场景视图生成

视图合成在视觉和图形方面都有悠久的历史。 一类方法通过两步过程合成场景的新颖视图。 这些方法首先估计输入视图的深度，并使用深度将输入图像扭曲到新视图。 然后，他们通过组合这些扭曲的图像来生成最终图像。 这些方法通常使用多视图立体算法（例如，Furukawa的PMVS）来估计深度并且不适合具有窄基线的光场。 在我们的系统中，我们还有深度和颜色估算组件。 但是，与这些方法不同，我们使用机器学习来模拟这两个组件。 此外，受Fitzgibbon方法的启发，我们通过直接最小化外观误差来训练我们的视差和颜色估计模型。

另一种常见方法是在不明确估计几何形状的情况下合成图像。 例如，Mahajan建议沿着特定路径移动输入图像中的渐变以在新视图处重建图像。 Shechtman提出了一种基于补丁的优化框架，用于在新视图中重建图像。 然而，这些方法不能利用光场中可用的所有信息，因为它们仅处理两个输入图像。

#### 2.3 立体视觉（DeepStereo）

Flynn最近提出了一种深度学习方法，用于在具有宽基线的图像序列上执行视图合成。 他们首先在多个深度平面上投影输入图像。 然后，他们根据这些投影图像估计每个深度平面处的图像的像素颜色和重量。 最后，他们计算估计的像素颜色的加权平均值以获得最终的像素颜色。 与此方法相比，我们的系统有几个关键差异。 首先，我们的方法专门设计用于光场，它具有更窄的基线和更常规的相机位置。 其次，与他们的方法不同，我们的系统明确地估计了可能在其他应用中使用的视差。 最后，我们的系统明显快于他们的方法（几分钟与秒）。 这显示了我们系统的效率，验证了更实用的用途。

#### 2.4 物体视图生成

自从最近发布3D形状模型的大数据集以来，从单个图像合成对象视图变得流行。 Kholgade从相应的3D模型转移纹理以呈现对象的新颖视图。 但是，需要手动注释来指定相应的3D模型及其在图像中的位置。 Su通过在数据集中选择几个类似的模型然后在它们之间进行插值来解决此限制。 但是，这些方法严重依赖于检索过程，并且在无法找到类似模型时会变得容易受到攻击。

最近，一些算法通过利用深度学习来解决这个问题。 Dosovitskiy训练CNN，一旦给出包含渲染细节的图形代码，就可以渲染椅子的图像。 Yang扩展了这项工作，并从输入图像中解码隐式渲染信息，而不是明确地将其表示为图形代码。 然后，他们应用所需的转换并渲染新视图。 Tatarchenko也采用了类似的方法，但没有明确地解除身份和姿势。 Zhou训练CNN估计视图流，然后用于将输入图像扭曲到新视图。 这些方法专门用于处理对象，并且在一般场景中不能很好地工作。 此外，它们仅使用单个图像，因此不能利用光场中的所有图像。

### 3. 拟议基于学习的方法

给定稀疏的输入视图集$L _ { p _ { 1 } } , \cdots , L _ { p _ { N } }$和新视图$q$的位置，我们的目标是估计新视图$L_q$处的图像。在形式上，我们可以这样写：

$L _ { q } = f \left( L _ { p _ { 1 } } , \cdots L _ { p _ { N } } , q \right)$

其中$p_i$和$q$分别表示输入和新视图的$(u,v)$坐标。 这里，$f$是定义输入视图和新视图之间的关系的函数。 这种关系通常非常复杂，因为它需要在所有输入视图之间找到联系，并根据新视图的位置从每个图像中收集适当的信息。 消费者光场相机中的诸如噪声和光学畸变之类的不准确性进一步增加了这种关系的复杂性。

因此，我们建议学习这种关系。 受近期深度学习在各种应用中的成功启发，我们建议使用卷积神经网络作为我们的学习模型。 一种直接的方法是使用CNN直接对函数$f$建模。 在这种情况下，CNN获取输入视图以及新视图的位置，并在新视图处输出图像。 但是，如图6所示，这种简单的解决方案通常会产生模糊的结果。 这主要是由于这种关系很复杂并且需要网络找到远距离像素之间的联系这一事实，这使得训练变得困难。

通过遵循现有视图合成技术的流程并将系统分解为视差和颜色估计组件，我们使训练更易于处理。 我们的主要贡献是使用机器学习对每个组件进行建模，并通过最小化合成图像和基本真实图像之间的误差来同时训练两个模型（参见第3.3节）。 在我们的系统中，我们首先根据从稀疏输入视图集中提取的一组特征来估计新视图中的视差：

$D _ { q } = g _ { d } ( K )$

其中$D_q$是新视图中的估计视差，$K$表示一组特征，包括不同视差水平下的扭曲图像的平均值和标准差（参见第3.1节）。 此外，$g_d$定义了输入特征与我们使用CNN建模的视差之间的关系，然后使用估计的视差来将输入图像扭曲到新视图。 具体地，我们通过基于新视图处的视差对输入图像进行采样来执行向后扭曲（参见等式4）。 最后，我们使用一组输入特征估计新视图中的图像，包括所有扭曲图像，估计视差和新视图的位置：

$L _ { q } = g _ { c } ( H )$

其中$H$代表我们的特征集，$g_c$定义了这些特征与最终图像之间的关系。 我们系统的概述如图3所示。在接下来的部分中，我们详细描述了视差估计器（公式2）和颜色预测器（公式3）。

![](/Users/sinyeratlantis/Desktop/pic/4-3.png)

图3：我们的系统由视差估计器和颜色预测器组件组成，我们使用两个连续的CNN进行建模。 在我们的系统中，我们首先从稀疏输入视图中提取一组特征。 然后，我们使用第一个CNN来估计新视图中的视差。 然后，我们使用此视差将所有输入视图扭曲（向后）到新视图。 我们的第二个CNN使用所有扭曲的图像以及一些其他功能来生成最终图像。

#### 3.1 视差估计

该组件的目标是估计新视图$D_q$的视差。 对于新颖视图图像的每个像素，该视差指向每个输入视图中的对应像素：

$\overline { L } _ { p _ { i } } ( s ) = L _ { p _ { i } } \left[ s + \left( p _ { i } - q \right) D _ { q } ( s ) \right]$

其中$s$是包含$x$和$y$方向上像素位置的向量。 此外，$p_i$和$q$也是包含$u$和$v$方向上的输入位置和新视图的向量。 这里，$\overline { L } _ { p _ { i } }$是通过使用视差$D_q$对输入视图$L _ { p _ { i } }$进行反向扭曲而获得的图像。 如果视差是准确的，则它将指向输入图像中的正确像素，因此，所有变形图像在每个像素处将具有相同的颜色。 然而，视差不是先验已知的，我们需要首先估计它。

为了估计视差，我们首先使用一组预定义的视差水平$d _ { 1 } , \cdots , d _ { L }$将所有输入图像扭曲（向后）到新视图，如下：

$\overline { L } _ { p _ { i } } ^ { d _ { l } } ( s ) = L _ { p _ { i } } \left[ s + \left( p _ { i } - q \right) d _ { l } \right]$

其中$i \in \{ 1 , \cdots , N \}$和$l \in \{ 1 , \cdots , L \}$。 在我们的实现中，我们在$[-21,21]$像素范围内使用$L = 100$个视差水平。 我们使用来自Tao的深度估计方法的思想，这也是其他近期技术的核心，从这些变形图像中提取一组特征。 具体来说，我们计算每个视差级别的所有扭曲输入图像的平均值和标准差，如下所示：

$M ^ { d _ { l } } ( s ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \overline { L } _ { p _ { i } } ^ { d _ { l } } ( s )$

$V ^ { d _ { l } } ( s ) = \sqrt { \frac { 1 } { N - 1 } \sum _ { i = 1 } ^ { N } \left( \overline { L } _ { p _ { i } } ^ { d _ { l } } ( s ) - M ^ { d _ { l } } ( s ) \right) ^ { 2 } }$

我们通过连接所有视差级别的均值和标准差来生成输入特征$K = \left\{ M ^ { d _ { 1 } } , V ^ { d _ { 1 } } , \cdots , M ^ { d _ { L } } , V ^ { d _ { L } } \right\}$。由于我们使用100个视差水平，我们的特征向量有200个通道。

如前所述，所有扭曲的输入视图都具有正确视差级别的照片一致性。 因此，现有技术通常选择具有最小标准偏差和最大平均对比度的视差水平。 由于从该过程获得的视差通常是有噪声的，因此这些方法使用优化方案来规范视差。 虽然这些方法产生高质量的视差图，但它们并不是专门为视图合成应用而设计的。 因此，如图10所示，它们通常在遮挡边界周围具有伪影，这是用于合成高质量图像的重要区域。

我们使用学习系统从输入要素估计最佳视差图来避免此问题。 如第二节所述，我们通过最小化估计的和真实的新颖视图图像之间的误差来训练我们的系统。 注意，可以通过最小化估计的和基本真实视差之间的误差来训练视差估计器。 但是，我们避免这种替代方案，因为它有两个主要缺点。 首先，以这种方式进行训练需要光场数据库及其相应的基本真实视差，这是难以获得的。 其次，如果最终目标是综合新颖的观点，那么视差并不总是准确的。 例如，即使具有不准确的视差，也可以容易地重建恒定颜色区域。

作为我们的模型，我们使用深层CNN，由四个卷积层组成，其中内核尺寸逐渐减小，如图5所示。除最后一层外，所有层后面都有一个整流线性单元。 接下来，我们解释我们的颜色预测器组件。

#### 3.2 颜色预测器

该组件的目标是使用由第一CNN估计的视差来估计最终颜色。 估计的视差可以用于使用等式4简单地将输入视图扭曲到新视图。现有视图合成技术具有组合这些扭曲图像并生成最终图像的特定方式。 例如，Chaurasia通过计算所有扭曲图像的加权平均值来获得最终图像。 然而，这些方法通常很简单，并且不能正确地模拟由于遮挡而变形的扭曲图像和最终合成图像之间的关系。

相反，我们建议学习这种关系。 我们从一组输入特征估计最终图像，包括所有扭曲图像，估计的视差和新视图的位置。 具体来说，我们的特征向量是$H = \left\{ \overline { L } _ { p _ { 1 } } , \cdots , \overline { L } _ { p _ { N } } , D _ { q } , q \right\}$。注意，视差对于检测遮挡边界并从这些区域附近的变形图像中收集适当的信息是有用的。 此外，新颖视图的位置可以潜在地用于在重建新视图时更多地加权特定图像。 例如，如果$q$接近$p_1$，则$\overline { L } _ { p _ { 1 } }$应该大量用于重建位置$q$处的新视图。 虽然我们没有明确地对遮挡进行建模，但是我们的系统学习通过依赖遮挡区域中具有有效信息的图像来重建最终图像。

在这里，我们使用与图5中类似的深度网络，具有不同数量的输入和输出。 在这种情况下，我们的输入有$3N + 3$个通道，输出是一个RGB图像，有3个通道。 在下一节中，我们将讨论训练系统的详细信息。

![](/Users/sinyeratlantis/Desktop/pic/4-5.png)

图5：我们的视差估计器网络由四个卷内核组成，内核大小减小。 所有层之后是整流线性单元（ReLU）。 我们的颜色预测器网络具有类似的架构，具有不同数量的输入和输出通道。 我们只使用卷积层来处理任何大小的图像。 我们凭经验发现，这种架构可以很好地模拟我们的组件并且速度相当快。

#### 3.3 训练

为了合成接近真实情况的高质量图像，我们通过最小化合成和基本真实图像之间的L2距离来训练网络：

$E = \sum _ { k = 1 } ^ { 3 } \left( \hat { L } _ { q , k } - L _ { q , k } \right) ^ { 2 }$

在求和于RGB通道上的情况下，$L _ { q , k }$是新视图中的基本真实图像，$\hat { L } _ { q , k }$是我们通过方程式获得的估计图像。 为了使用基于梯度下降的技术来最小化我们的能量函数，我们需要计算方程中误差关于两个网络权重的导数，即$\partial E / \partial w _ { d }$和$\partial E / \partial w _ { c }$，其中$w_d$和$w_c$是矢量，并且分别指代视差和颜色估计器网络的所有权重。

由于颜色预测器网络直接输出合成图像，因此可以像在标准反向传播中那样容易地计算$\partial E / \partial w _ { c }$。 对于$\partial E / \partial w _ { d }$，我们使用链式法则将导数分解为三个项，如下所示：

$\frac { \partial E } { \partial w _ { d } } = \sum _ { k = 1 } ^ { 3 } \left[ \frac { \partial E } { \partial \hat { L } _ { q , k } } \frac { \partial \hat { L } _ { q , k } } { \partial D _ { q } } \right] \frac { \partial D _ { q } } { \partial w _ { d } }$

由于我们的误差是二次的，因此可以很容易地计算第一项。 最后一项是视差估计网络的输出相对于其权重的导数，其可以照常计算。 中间项是最终图像相对于估计视差的导数。 请注意，视差用于生成一组特征$H$（参见第3.2节）。 然后，颜色估计器网络使用这些特征来产生最终图像。 因此，我们有：

$\frac { \partial \hat { L } _ { q , k } } { \partial D _ { q } } = \sum _ { t = 1 } ^ { 3 N + 3 } \frac { \partial \hat { L } _ { q , k } } { \partial H _ { t } } \frac { \partial H _ { t } } { \partial D _ { q } }$

其中总和在特征向量的各个通道上。 这里，第一项是颜色预测器网络输出相对于其输入的导数，并且可以直接计算。 对于第二项，我们需要分别调查每个通道。 我们的输入特征向量$H$的前3$N$个通道是扭曲图像，因此，$\partial H _ { t } / \partial D _ { q }$基本上是方程4中扭曲函数的导数。幸运的是，由于我们使用双三次插值来计算颜色值，所以这个函数是可微分的。 为了简化实现，我们在数值上计算这个梯度。 下一个通道$H _ { 3 N + 1 }$的特征是估计的深度，其导数等于1。 最后，最后两个通道是新视图的位置，其独立于视差，因此，它们的梯度等于零。

![](/Users/sinyeratlantis/Desktop/pic/4-4.png)

图4：我们在SEAHORSE场景中显示单个视差水平的均值和标准差特征。 该特定视差水平对于前景是正确的。 结果，平均图像中的海马是锐利的，而背景是模糊的。 而且，海马的标准偏差很小，背景也很大。 我们的网络学习使用这些功能来估计每个像素的正确视差。

在训练的每次迭代中，我们使用这些梯度来更新梯度相反方向上的网络权重。 我们在训练集中使用了Lytro Illum相机拍摄的100个光场。 为了处理多样化的测试集，我们确保我们的训练集包含各种不同的场景，包括自行车、汽车、树木和树叶（参见补充材料）。 我们自己拍摄了大部分这些图像，并从Raj的数据集中获取了一些图像。 这些光场的角分辨率为8×8，我们仅使用四个角子孔径图像作为输入。 对于每个光场，我们从原始的8×8网格中随机选择了四个新的视图位置。 对于每个新颖的视图位置，我们提取了一组特征（参见方程6）并使用该位置处的原始捕获图像作为基本真实图像。

由于对完整图像的训练很慢，我们从整个图像中提取了大小为60×60的片段，其中步长为16像素，这导致我们使用了100,000多个补丁训练我们系统。 请注意，对于每个输入补丁，我们的系统输出一个36×36大小的补丁（缩小尺寸是由于卷积），然后将这些输出补丁与基本真实补丁进行比较，并且反向传播每个像素处的误差以训练网络。 因此，在实践中，我们有超过100,000,000个例子，我们发现这些例子足以正确训练两个网络。 我们使用大小为20的小批次在速度和收敛之间取得最佳平衡。 我们使用Xavier方法随机初始化我们的网络权重，并使用ADAM求解器训练我们的系统，$β_1= 0.9$，$β_2= 0.999$，学习率为0.0001。

### 4. 结果

我们在MATLAB中实现了我们的方法，并使用MatConvNet来实现我们的网络。 此处显示的所有结果都是在使用Lytro Illum相机拍摄的光场上生成的。 捕获光场的角分辨率为8×8，我们仅使用四个角子孔径图像作为输入来生成全光场。 请注意，我们的方法可以生成任何中间视图。 但是，我们只生成8×8视图，以便能够将它们与基本真实图像进行比较。 这里，我们只为每个场景显示一个合成图像$(5,5)$，但是可以在补充视频中找到显示所有视图的视频。

#### 4.1 与单个网络的比较

我们首先比较使用图6中的单个CNN对过程进行建模的结果。这里，网络直接模拟输入图像和新视图之间的关系（参见等式1）。 然而，这种关系很复杂，并且需要网络经常考虑远处像素的联系，这使得训练变得困难。 因此，与包含两个连续CNN的架构相比，单个CNN的结果是模糊的并且包含伪像。 例如，单个CNN不能在输入视图中联系白色卡车的像素，因此产生具有重影伪像的结果。

#### 4.2 与其他方法的比较

接下来，我们将我们的方法与Wanner和Goldluecke的方法进行比较。 他们首先使用现有技术计算每个输入视图的视差。 然后，他们使用优化框架内的视差通过最小化目标函数来获得新颖的视图。 我们采用了几种最先进的光场视差估计方法来生成Wanner和Goldluecke方法所需的视差。 具体来说，我们使用了Wanner和Goldluecke、Tao、Wang和Jeon的方法。 我们在PSNR和结构相似性（SSIM）方面以数字方式评估结果。 SSIM产生0到1之间的值，其中1表示相对于基本真实的完美感知质量。

表1显示了30个测试场景中所有方法的平均PSNR和SSIM值。为了在具有挑战性的案例中正确评估我们的系统，我们在大约一半的测试集中使用了叶子和花的图像。请注意，我们有完全独立的训练和测试集，并且没有任何测试场景是训练集的一部分（参见补充材料）。如图所示，我们的方法产生的结果明显优于其他方法。我们在图7中展示了其中的四个场景。花1场景展示了卡车，建筑物和树木（右侧）前面的花朵。花和叶具有复杂的结构，这使得其他方法难以准确地估计边界处的视差。因此，它们的结果通常包含遮挡边界周围的伪影。然而，我们的方法产生了合理的结果，这种结果与基本真实图像相当接近。请注意，例如，只有我们的方法能够正确地重建卡车的车顶（绿色插图）和高光（蓝色插图）。

接下来，我们检查CARS场景，显示街道前面的树枝。 尽管场景简单，但是其他方法通常不能仅从四个输入图像精确地估计分支边界周围的视差。 因此，它们的结果包含撕裂伪影，可以在蓝色插图中特别看到。 此外，用于合成新视图的Wanner和Goldluecke的方法并未模拟消费者光场相机的不准确性，这些相机通常在结果中显示为变色（参见红色插图中的彩色像素）。 请注意，只有我们的方法能够重建遮挡边界周围的所有细节，例如红色插图中的细垂直线。

FLOWER 2场景包含一条在街道前面具有复杂结构的花。 我们的方法产生了比其他方法更好的合理结果。 请注意，只有我们的方法能够忠实地重建花茎和花瓣之间的挑战性区域（蓝色插图）。 最后，ROCK场景对于所有其他方法都很困难。 它们通常无法准确估计岩石边界周围的视差，从而导致撕裂伪影。 与此同时，相对于基本事实，我们产生的结果比其他方法更好。

总的来说，所有其他方法都显示了遮挡边界周围的撕裂、重影和其他伪影，这是视图合成应用的重要区域。 主要原因是这些方法并非专门针对此应用而设计，因此，它们通常在这些边界周围存在不准确性。 此外，用于生成新颖视图的Wanner和Goldluecke的方法假设图像是在理想条件下捕获的，而消费者光场相机则不是这种情况。 另一方面，我们的方法产生了合理的结果，这些结果与实际情况相当接近。 从数字上看，我们的结果明显优于其他方法。

我们将我们的方法与图8中具有挑战性的场景上的其他方法进行比较。该场景包含大量遮挡区域，这些遮挡区域通常难以进行视图合成。 因此，即使我们的方法也难以在困难区域中合成高质量图像（参见图12）。 但是，我们的结果总体上是合理的，并且明显优于所有其他方法。 请注意，叶子具有薄的结构，只有我们的方法才能正确地重建它们，而不会在背景中引入伪影（绿色和蓝色插图）。

我们还将我们的方法与图9中最近的Zhang方法进行了比较。请注意，他们的方法需要一些用户交互，而我们的方法是全自动的。 而且，他们的方法需要中心视图，因此使用五个输入图像（而不是四个）。 然而，即使用户交互，他们的方法也不能将场景正确地分解成不同的深度层，从而导致撕裂伪像。

#### 4.3 计时

我们的方法需要大约12.3秒才能在具有16 GB内存和GeForce GT 730 GPU的英特尔四核3.4 GHz机器上从分辨率为541×376的四个输入图像生成新视图。 具体地，提取特征需要5.5秒，评估视差估计网络需要5.1秒，将四个输入图像扭曲到新视图需要0.2秒，评估颜色预测网络需要1.5秒。

#### 4.4 分析我们的系统

我们评估每个组件在我们系统中的效果。图10比较了我们估计的视差与SEAHORSE场景的其他方法（如图2所示）。尽管一些其他方法产生的视差具有比我们更高的质量，但是它们的视差通常具有围绕遮挡边界的伪影，遮挡边界是视图合成的最重要区域。例如，这些方法不能适当地估计绿色插图中间的背景的视差，或红色插图中的海马鼻子的边界。因此，他们经常人为估计这些区域，这可以在我们的补充视频中看到。

另一方面，我们的方法可以在这些区域产生合理的视差。请注意，我们的方法并不总能产生准确的视差。但是，我们的不准确性通常发生在对视图合成不重要的区域。例如，部分海马鼻子在我们的视差中被错误地检测为背景（红色插图的左侧部分中的白色区域）。然而，这是一个恒定的颜色区域，因此，这种不准确性不会影响合成结果的质量（见图2）。这是因为我们通过直接最小化合成和基本真实图像之间的误差来训练我们的视差估计器网络。将来，将我们的学习方案与现有视差估计方法的想法相结合以产生更准确的视差将是有趣的。

接下来，我们评估图11中颜色预测器网络的效果。这里，我们分别在顶部和底部显示FLOWER 1和CARS场景的插图。 我们使用新视图中的估计视差来扭曲所有输入视图到新视图。 由于遮挡，这些扭曲的图像通常包含伪影，如红色箭头所示。 我们的色彩预测器网络可以正确地检测这些区域，并通过从变形图像中收集适当的信息来生成高质量的图像。

#### 4.5 去噪效果

由于我们使用所有输入视图来生成新颖的视图，因此与基本真实图像相比，我们的结果通常较少噪声。 我们推荐读者到我们的补充视频中查看此效果，这对于在噪声很小的低光条件下捕获光场可能是有用的。

#### 4.6 限制

我们的颜色预测器网络使用扭曲图像生成最终图像。 因此，在没有任何扭曲图像包含有效信息的情况下，我们的方法无法产生高质量的结果。 对于LEAVES场景，其中一种情况如图12所示。 在这里，我们的方法无法在叶子之间合成视图并不产生撕裂伪影。 但是，我们的结果比其他方法要好得多。

此外，如补充视频所示，我们的方法可用于外推。 但是，由于我们专门训练我们的网络进行插值，因此我们的外推结果通常质量较低。 然而，我们的方法仍然比其他方法产生更好的结果。

最后，虽然在本文中我们关注的是消费相机获得的光场，但我们相信类似的架构可以适应具有较大视差的非结构化光场。 但是，与任何基于学习的技术一样，我们的系统需要重新训练才能适应这些情况。

### 5. 总结及未来工作

我们提出了一种新颖的基于学习的方法，用于从用消费者光场相机捕获的稀疏输入视图集合来合成新颖视图。我们的系统由视差和颜色估计器组件组成，我们使用两个顺序卷积神经网络进行建模。我们仅使用Lytro Illum相机拍摄的四个角落子孔径图像，在各种场景上显示我们的方法的结果。实验结果表明，我们的方法优于最先进的方法。

将来，我们想研究使用我们的系统从一组具有不同曝光的视图生成高动态范围光场的可能性。此外，扩展我们的系统以使用任意数量的输入视图将是有趣的。我们也有兴趣提高算法的速度，以便可以以交互速率甚至实时工作。最后，有可能使用我们的系统以及光场压缩方案来增加压缩比，例如，通过从稀疏集生成新颖视图并压缩视差。