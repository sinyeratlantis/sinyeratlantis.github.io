<!doctype html>
<html style='font-size:22px !important'>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>视频分析文献调研</title><link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color: #ffffff; --text-color: #333333; --select-text-bg-color: #B5D6FC; --select-text-font-color: auto; --monospace: "Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857143; overflow-x: hidden; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit; background-position: inherit inherit; background-repeat: inherit inherit; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; word-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) { 
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:first-child { margin-top: -20px; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.701961); color: rgb(85, 85, 85); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 80px; }
.CodeMirror-gutters { border-right-width: 0px; background-color: inherit; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit; position: relative !important; background-position: inherit inherit; background-repeat: inherit inherit; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; background-position: 0px 0px; background-repeat: initial initial; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print { 
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid-page; break-before: avoid-page; }
  #write { margin-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 1cm; padding-right: 1cm; padding-bottom: 0px; break-after: avoid-page; }
  .typora-export #write::after { height: 0px; }
  @page { margin: 20mm 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background-color: rgb(204, 204, 204); display: block; overflow-x: hidden; background-position: initial initial; background-repeat: initial initial; }
p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; text-align: center; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-top-left-radius: 10px; border-top-right-radius: 10px; border-bottom-right-radius: 10px; border-bottom-left-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) { 
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background-color: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-top-left-radius: 3px; border-top-right-radius: 3px; border-bottom-right-radius: 3px; border-bottom-left-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; background-position: initial initial; background-repeat: initial initial; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; }
a.md-print-anchor { white-space: pre !important; border: none !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; text-shadow: initial !important; background-position: 0px 0px !important; background-repeat: initial initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom-width: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

@font-face {
    font-family: 'Open Sans';
    font-style: normal;
    font-weight: normal;
    src: local('Open Sans Regular'),url('file:///Users/sinyeratlantis/Library/Application%20Support/abnerworks.Typora/themes/github/400.woff') format('woff');
}

@font-face {
    font-family: 'Open Sans';
    font-style: italic;
    font-weight: normal;
    src: local('Open Sans Italic'),url('file:///Users/sinyeratlantis/Library/Application%20Support/abnerworks.Typora/themes/github/400i.woff') format('woff');
}

@font-face {
    font-family: 'Open Sans';
    font-style: normal;
    font-weight: bold;
    src: local('Open Sans Bold'),url('file:///Users/sinyeratlantis/Library/Application%20Support/abnerworks.Typora/themes/github/700.woff') format('woff');
}

@font-face {
    font-family: 'Open Sans';
    font-style: italic;
    font-weight: bold;
    src: local('Open Sans Bold Italic'),url('file:///Users/sinyeratlantis/Library/Application%20Support/abnerworks.Typora/themes/github/700i.woff') format('woff');
}

html {
    font-size: 16px;
}

body {
    font-family: STKaiti, Times;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}
#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    /*background: url("file:///Users/sinyeratlantis/Library/Application%20Support/images/modules/styleguide/para.png") no-repeat 10px center;*/
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    padding-bottom: .3em;
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
   padding-bottom: .3em;
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}
h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}
table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}
table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}
table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 4px 2px 4px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding: 0.2em 1em;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
}

 .typora-export p, .typora-export .footnote-line {white-space: normal;} 
</style>
</head>
<body class='typora-export' >
<div  id='write'  class = 'is-mac'><h2><a name='header-n0' class='md-header-anchor '></a>基于如何利用游戏AI中的时序视觉信息所做的调研</h2><h3><a name='header-n2' class='md-header-anchor '></a>1. action recognition</h3><p>这类问题通常是以监督学习的方式进行，所使用的数据库是剪辑好的（trimed）视频-动作对，可以看做是视频层面的image classification问题。</p><h4><a name='header-n4' class='md-header-anchor '></a>1.1 传统方法</h4><ol start='' ><li><p>DT算法：Dense Trajectories and Motion Boundary Descriptors for Action Recognition, IJCV2013</p><p>DT算法的基本思路为利用光流场来获得视频序列中的一些轨迹，再沿着轨迹提取HOF，HOG，MBH，trajectory这4种特征，其中HOF基于灰度图计算，另外几个均基于dense optical flow（密集光流）计算。最后利用FV（Fisher Vector）方法对特征进行编码，再基于编码结果训练SVM分类器。</p></li><li><p>iDT算法：Action recognition with improved trajectories, ICCV2013</p><p>iDT算法为DT算法的改进。iDT改进的地方在于它利用前后两帧视频之间的光流以及SURF关键点进行匹配，从而消除/减弱相机运动带来的影响，改进后的光流图像被成为warp optical flow。</p></li></ol><h4><a name='header-n12' class='md-header-anchor '></a>1.2 深度学习方法</h4><ol start='' ><li><p>双流网络：Two-Stream Convolutional Networks for Action Recognition in Videos, NIPS2014</p><p>基本思路是对视频序列中每两帧计算密集光流，得到密集光流的序列（作为temporal信息），然后对于视频图像（spatial）和密集光流（temporal）分别训练CNN模型，两个分支的网络分别对动作的类别进行判断，最后直接对两个网络的class score进行fusion（包括直接平均和svm两种方法），得到最终的分类结果，两个分支使用的是相同的2D CNN网络结构。</p></li><li><p>Convolutional Two-Stream Network Fusion for Video Action Recognition, CVPR2016</p><p>双流网络的提升，fusion部分采用了CNN的方式，并且spatial和temporal网络都换成了VGG-16 network。</p></li><li><p>TSN：Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV2016</p><p>港中文Limin Wang的工作，依旧是承接双流网络的思想，提出了稀疏采样的策略，在避免冗余信息的同时可以处理长距离依赖。</p><p>具体来说，TSN把视频分成3段，每个片段均匀地随机采样一个视频片段，并使用双流网络得到视频片段的各类得分（softmax之前的值），之后把不同片段的得分取平均，最后通过softmax输出。训练时，TSN可以根据整个视频的信息而不只是一个视频片段的信息对网络参数进行学习。TSN获得了ActivityNet 2016竞赛的冠军(93.2% mAP)。</p><p>另外，TSN也提出了一些tricks：</p><ul><li>输入数据：除去two stream的RGB image和 optical flow field这两种输入外，文章还尝试了RGB difference及 warped optical flow field的两种输入，最终结果是RGB + optical flow + warped optical flow的组合效果最好。</li><li>网络结构：测试了GoogLeNet，VGGNet-16及BN-Inception三种网络结构，其中BN-Inception的效果最好。</li><li>训练策略：尝试了跨模态预训练，正则化，数据增强等。</li></ul></li><li><p>C3D：Learning spatio-temporal features with 3d convolutional networks, ICCV2015</p><p>C3D是facebook的一个工作，网络采用3D卷积以及3D Pooling，避开了optical flow的提取，使得网络的训练大大加快，这个工作已经有了后续的更新。</p></li></ol><h3><a name='header-n35' class='md-header-anchor '></a>2. temporal action detection</h3><p>现实中的视频大多都是未剪辑的，所以这部分讨论的问题就是，如何给出视频动作的开始和结束，及其类别。</p><p>这部分与图像里的目标检测有些类似，方法也是大多是基于R-CNN系算法在视频上的扩展，工作也不太多，我也不太感兴趣，未加深入。</p><ol start='' ><li><p>End-to-end learning of action detection from frame glimpses in videos, CVPR2016</p><p>李飞飞实验室的工作。这篇文章使用强化学习的方法训练了一个基于RNN的agent，这个agent不断观察视频帧并不断决定接下来要看哪里以及什么时候要生成一个动作预测。与后面很多基于proposal的方法不同，该方法是end-to-end且是直接生成行为预测的。</p></li><li><p>SCNN：Temporal action localization in untrimmed videos via multi-stage cnns, CVPR2016</p><p>SCNN首先使用滑窗的方法生成多种尺寸的视频片段(segment)，再使用多阶段的网络(Segment-CNN)来处理。</p><p>具体而言，SCNN主要包括三个子网络，均使用了C3D network。</p><ul><li>proposal network：用来判断当前输入的视频片段是一个动作的概率。</li><li>classification network：用于给视频片段分类，但该网络不用于测试环节，而只是用作初始化localization network。</li><li>localization network：依旧是输出类别的概率，但在其训练时加入了重叠度相关的损失函数，使得网络能更好的估计一个视频片段的类别和重叠度。</li></ul><p>最后采用了非极大化抑制（NMS）来去除重叠的片段，完成预测。</p><p>该方法实际上采用了类似于R-CNN的思路，之后也有不少文章采用了类似的思想，即先提proposal，再分类。</p></li><li><p>CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos, CVPR2017</p><p>类似于语义分割问题的思路，基于C3D（3D CNN网络）设计了一个卷积逆卷积网络，输入一小段视频，输出frame-level的动作类别概率。该网络主要是用来对temporal action detection中的动作边界进行微调，使得动作边界更加准确，从而提高mAP。由于基于了层数不多的C3D网络，该方法的速度非常快，可以达到500FPS。</p></li></ol><h3><a name='header-n59' class='md-header-anchor '></a>3. video captioning</h3><p>对应于给图像生成一段描述，描述视频意味着对其中时序信息的准确把握。</p><ol start='' ><li><p>Translating Videos to Natural Language Using Deep Recurrent Neural Networks</p><p>基本思路：利用cnn将视频中所有帧提取特征（fc7）然后进行mean pool之后送入后面的LSTM decoder进行文字的生成。</p></li><li><p>S2VT：Sequence to Sequence – Video to Text, ICCV2015</p><p>S2VT是对上一篇文章模型的改进，由于直接进行mean pool会忽略掉这些特征的顺序和时序关系，所以作者将每帧的特征都按顺序先送到LSTM encoder中，然后再送到decoder中生成文字。</p></li><li><p>C3D：Learning Spatio-Temporal Features with 3D Convolutional Networks, ICCV2016</p><p>此前已提到。<a href='https://github.com/facebook/C3D' target='_blank' class='url'>https://github.com/facebook/C3D</a></p></li><li><p>Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation, ACMMM2016</p><p>MSR-VTT Challenge 2016的第一名，文章先用多个基于不同特征的video caption方法（均为encoder-decoder结构）对视频生成多段描述，再构造了一个基于CNN的评价网络，其中输入为video caption方法得到的句子和视频的特征，输出为两者之间的匹配度。这个评价网络实际上可以视为多个video caption模型的ensemble方法。</p></li><li><p>Multi-Task Video Captioning with Video and Entailment Generation, ACL2017</p><p>很有意思的一个工作，整个模型包括三个模块：</p><ul><li>Unsupervised video prediction：无监督的未来帧预测，使用video encoder和video decoder。</li><li>Entailment Generation：句子含义生成，也就是给句子生成含义相似的新句子，使用language encoder和 language decoder。</li><li>Video Captioning：视频语义生成，使用了video encoder和language decoder。</li></ul><p>其中，video encoder和laguange decoder的参数是共享的。</p></li><li><p>P3D：Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks, ICCV2017</p><p>微软亚洲研究院多媒体搜索与挖掘组的工作，用一个1×3×3的空间方向卷积和一个3×1×1的时间方向卷积近似原3×3×3卷积。通过组合三种不同的模块结构，进而得到P3D ResNet。P3D ResNet在参数数量、运行速度等方面对C3D作出了优化。卷积可以利用2d resnet的参数来初始化。</p></li><li><p>I3D：Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset, CVPR2017</p><p>发布了大名鼎鼎的trimmed video dataset：Kinetics。论文还提出了Inflating 3D ConvNets，梳理了各个时期用深度学习来做行为识别的典型方法并做了很多实验验证，值得一看。</p><p><img src='pic/contrast.png' alt='' referrerPolicy='no-referrer' /></p><p>文章将在ImageNet训好的2D模型参数展开成3D，之后再做训练，因此叫Inflating 3D ConvNets。具体还有许多细节，参见论文。</p></li><li><p>Weakly Supervised Dense Video Captioning, CVPR2017</p><p>dense video captioning是要对一段视频产生所有可能的描述。本文提出了一种弱监督方法，Multi-instance multi-label learning（MIMLL) 。MIMLL直接从视频-句子对数据中学习每个视频对应的词汇向量，之后将这些词汇向量结合起来作为encoder-decoder的输入，实现video captioning。</p></li><li><p>Non-local Neural Networks, CVPR2018</p><p>Xiaolong Wang在CMU和FAIR的工作，可以看作是3D卷积的一个扩展。3D卷积的感受野是有限区域，而non-local旨在解决长距离依赖问题，Non-local的响应是所有空间和时间位置特征的加权平均。代码已开源。</p><p><a href='https://github.com/facebookresearch/video-nonlocal-net' target='_blank' class='url'>https://github.com/facebookresearch/video-nonlocal-net</a></p></li><li><p>Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? , CVPR2018</p><p>在Kinetics上从零训练resnet，比较新的工作，代码已开源。</p><p><a href='https://github.com/kenshohara/3D-ResNets-PyTorch' target='_blank' class='url'>https://github.com/kenshohara/3D-ResNets-PyTorch</a></p></li><li><p>Reconstruction Network for Video Captioning, CVPR2018</p><p>将Decoder产生的输出送入Reconstructor（LSTM/GRU）中重构，得到输出向量后再和Encoder输出（即Decoder输入）做L2 loss。类似于自编码器，将video to sentence后再sentence to video， dual learning，使得中间的特征学习的更好。  </p></li></ol><p>在caption时也可以加入音频特征，这部分我还未深入，可以先参考Google的两篇文章及其开源代码：</p><ul><li>AudioSet: An ontology and human-labelled dataset for audio events, ICASSP 2017</li><li>CNN Architectures for Large-Scale Audio Classification, ICASSP 2017</li><li><a href='https://github.com/tensorflow/models/tree/master/research/audioset' target='_blank' class='url'>https://github.com/tensorflow/models/tree/master/research/audioset</a></li></ul><p>Attention Mechanism也可以考虑在caption上做一些工作。</p><p>另外还有用强化学习做video captioning，这部分可以作为一个方向：</p><ul><li>Deep Reinforcement Learning-based Image Captioning with Embedding Reward, CVPR2017</li><li>Reinforced Video Captioning with Entailment Rewards, EMNLP2017</li></ul><h3><a name='header-n122' class='md-header-anchor '></a>数据集</h3><ul><li><p>HMDB51</p><p>HMDB: A large video database for human motion recognition, ICCV2011</p><p><a href='http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar' target='_blank' class='url'>http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar</a></p></li><li><p>UCF101</p><p>UCF101: A dataset of 101 human action classes from videos in the wild</p><p><a href='http://crcv.ucf.edu/data/UCF101/UCF101.rar' target='_blank' class='url'>http://crcv.ucf.edu/data/UCF101/UCF101.rar</a></p></li><li><p>Sports-1M（大）</p><p>Large-scale video classification with convolutional neural networks, CVPR2014</p><p>Karpathy的工作。</p></li><li><p>ActivityNet（大）</p><p>ActivityNet: A large-scale video benchmark for human activity understanding, CVPR2015</p></li><li><p>Kinetics（非常大）</p><p>The Kinetics human action video dataset</p></li><li><p>YouTube-8M（非常非常大）</p><p>What actions are needed for understanding human actions in videos?, ICCV2017</p></li></ul><p>video captioning常用数据集：</p><ul><li><p>MSR-VTT</p></li><li><p>MSVD</p><p><a href='http://www.cs.utexas.edu/users/ml/clamp/videoDescription/YouTubeClips.tar' target='_blank' class='url'>http://www.cs.utexas.edu/users/ml/clamp/videoDescription/YouTubeClips.tar</a></p><p><a href='https://www.microsoft.com/en-us/download/confirmation.aspx?id=52422' target='_blank' class='url'>https://www.microsoft.com/en-us/download/confirmation.aspx?id=52422</a></p></li></ul><h3><a name='header-n153' class='md-header-anchor '></a>其他参考论文</h3><ul><li><p>3D Convolutional Neural Networks for Human Action Recognition, TPAMI2013</p><p>提出了3d卷积网络。</p></li></ul><ul><li><p>Zero-Shot Learning - The Good, the Bad and the Ugly, CVPR2017</p><p>关于零样本学习的一篇综述。</p></li><li><p>Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification, IJCAI2018</p><p>视频表示学习的自监督学习的无监督范式。</p></li><li><p>Dense-Captioning Events in Videos, CVPR2017</p><p>李飞飞组的工作，提出了activitynet的captioning数据集并给出一个baseline。</p></li><li><p>Temporal Action Detection with Structured Segment Networks, ICCV2017</p><p>TAG或称SSN，扩展行为定位的范围然后做回归，很工程性的工作。</p></li><li><p>Graph Distillation for Action Detection with Privileged Information, ECCV2018</p><p>李飞飞组的工作，一作Zelun Luo值得关注一下。文章关注点在弱监督学习在视频分析中的应用，比如领域适应和零样本学习，是一个很好的方向。</p><p>具体细节可见：<a href='https://zhuanlan.zhihu.com/p/31692742' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/31692742</a></p></li><li><p>A Closer Look at Spatiotemporal Convolutions for Action Recognition, CVPR2018</p><p>facebook的工作，研究C3D参数的减少。</p></li><li><p>Rethinking Spatiotemporal Feature Learning For Video Understanding, CVPR2018</p><p>google的工作，研究C3D参数的减少。</p></li><li><p>online action detection相关的研究：</p><ul><li>Online Action Detection, ECCV2016</li><li>RED: Reinforced Encoder-Decoder Networks for Action Anticipation, BMVC2017</li><li>Online Detection of Action Start in Untrimmed, Streaming Videos</li><li>Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs</li></ul></li></ul><h3><a name='header-n193' class='md-header-anchor '></a>Reference</h3><p>这方面讨论也比较多，paper在前面都放过了，这里仅列一些写总结时参考过的文章。</p><ul><li><p><a href='https://www.zhihu.com/question/64021205' target='_blank' class='url'>https://www.zhihu.com/question/64021205</a></p><p>知乎该问题下所有回答，特别参考了高继扬，Xiaolong Wang，以及qjzhao的回答</p></li><li><p><a href='https://zhuanlan.zhihu.com/wzmsltw' target='_blank' class='url'>https://zhuanlan.zhihu.com/wzmsltw</a></p><p>林天威的专栏</p></li><li><p><a href='https://zhuanlan.zhihu.com/p/36330561' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/36330561</a></p><p>Hao Zhang的视频理解近期研究进展</p></li><li><p><a href='https://zhuanlan.zhihu.com/p/36818044' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/36818044</a></p><p>图图的林达华老师valse 2018总结</p></li><li><p><a href='https://zhuanlan.zhihu.com/p/35730675' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/35730675</a></p><p>高继扬的Video Online Action Detection &amp; Anticipation梳理与探讨</p></li></ul></div>
</body>
</html>