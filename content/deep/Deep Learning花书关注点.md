## 《Deep Learning》可关注点

1. 前言

2. 线性代数

   奇异值分解与Moore-Penrose 伪逆

   主成分分析：

   输入通过一个编码器函数压缩，然后通过解码器解码

   PCA使用矩阵点乘作为解码器函数，并且限制矩阵的列向量彼此正交

   在PCA算法中，编码器的输出的效果衡量通过与输入的L2范数

   最小化L2范数函数的微分闭式解为：编码输出 = 解码矩阵的转置点乘输入

   最后我们可以得出输出为：矩阵\*矩阵转置\*输入

   我们用Frobenius范数（矩阵的L2范数）衡量输入与最终输出的距离

   对于范数的求解，需要给定编码矩阵的维度

   当我们给定矩阵的维度为1时，我们就得出了PCA的第一个主成分（X转置\*X的最大特征值对应的特征向量）

   主成分的基为矩阵前l个最大特征值对应的特征向量

3. 信息论

   KL散度：用于衡量两个概率分布之间的差异（对数差的均值）

   交叉熵函数：消除KL散度中输入概率那一项，对输出最小化

   结构化概率模型或图模型：综合概率分布等于各个单独概率分布或单独条件概率分布的乘积

4. 数值计算

   Jacobian矩阵：输出对于输入的所有一阶偏导数

   Hessian矩阵：输出对于输入的所有二阶偏导数合并成的矩阵

   使用一阶迭代的方法称为梯度下降

   牛顿法：通过一个二阶泰勒展开计算出函数的临界最小值

   - 添加约束简化优化：

     Lipschitz连续：norm1/norm2小于等于Lipschitz常数

     凸优化：Hessian处处半正定的函数

   - 在给定约束条件下的优化：转化为无约束的优化问题

     Karush–Kuhn–Tucker(KKT)方法：广义 Lagrange 函数(generalized Lagrange function)

     可以看看线性最小二乘的优化

5. 机器学习

   统计学习理论(statistical learning theory)假设训练集与测试集符合独立同分布，称为数据生成过程(data generating process)的概率分布生成

   - Vapnik-Chervonenkis维度(Vapnik-Chervonenkis dimension, VC)：

     统计学习理论提供的量化模型容量的一种方法。假设存在 m 个不同x点的训练集，分类器可以任意地标记该 m 个不同的x点，VC 维被定义为m的最大可能值。

   从预先知道的真实分布 p( x, y) 预测而出现的误差被称为贝叶斯误差(Bayes error)

   贝叶斯统计与最大后验估计

   SVM中的核函数与高斯核（径向基函数）

   - 无监督学习：降维提取信息的方法

     低维表示、稀疏表示和独立表示

     PCA与K-均值聚类

     局部不变性与平滑正则化

     流形学习：概率质量高度集中

6. 深度前馈网络

   反向传播相关


7. 深度学习中的正则化

   半监督学习及多任务学习

   稀疏表示

   Bagging（模型平均）：Boosting：添加并集成神经网络

   Bagging集成必须根据所有成员的累积投票做一个预测。在这种背景下，我们将这个过程称为推断(inference)。

   对抗训练，切面距离，正切传播，流形正切分类器


8. 深度模型中的优化

   长期依赖，非精确梯度，局部和全局结构间的弱对应

   动量：Nesterov动量等

   自适应学习率算法：Delta-bar-delta算法（早期），AdaGrad算法，RMSProp 算法（AdaGrad修改），Adam

   二阶近似方法：牛顿法，共轭梯度，Broyden-Fletcher-Goldfarb-Shanno(BFGS)算法，存储受限的BFGS(L-BFGS)

   优化策略和元算法：批标准化，坐标下降（单维分离），Polyak平均，监督预训练，延拓法和课程学习


9. 卷积网络

10. 循环神经网络

11. 实践方法论

    性能度量：PR曲线：以精度和召回率衡量模型效果的方法


12. 应用

    CPU/GPU实现

    分布式：异步随机梯度下降

    模型压缩

    动态结构


13. 线性因子模型

14. 自编码器

15. 表示学习

    迁移学习和领域自适应


16. 深度学习中的结构化概率模型（图模型）

    有向模型/信念网络/贝叶斯网络

    无向模型/马尔可夫随机场/马尔可夫网络

    配分函数：使无向模型中的概率函数归一

    受限玻尔兹曼机

17. 蒙特卡洛方法

    用于采样和蒙特卡洛估计

    马尔可夫链蒙特卡罗(Markov Chain Monte Carlo, MCMC)方法

    Gibbs 采样

    回火(tempering)

18. 直面配分函数

19. 近似推断

    期望最大化(expectation maximiza- tion, EM)算法

    最大后验推断和稀疏编码

20. 深度生成模型

    玻尔兹曼机

    受限玻尔兹曼机

    深度信念网络

    深度玻尔兹曼机


