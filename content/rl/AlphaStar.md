# AlphaStar DeepMind公开博客

AlphaStar: Mastering the Real-Time Strategy Game StarCraft II

## 个人总结

AlphaStar首先是通过暴雪的监督数据训练一个神经网络，使这个智能体就足以95%击败内置AI。然后使用了多智能体强化学习方法，基于这个智能体创建很多个agent，然后对战，产生大量对战数据供每个agent学习。如果未来论文没有什么新颖的部分出现，那么可以理解为这次突破是大量算力强行突破了。

## 博客翻译

### 摘要

几十年来，游戏一直被用作测试和评估人工智能系统性能的重要方法。 随着能力的提高，研究界开始寻求越来越复杂的游戏，捕捉解决科学和现实问题所需的不同智能元素。 近年来，星际争霸被认为是最具挑战性的即时战略（RTS）游戏之一，也是有史以来玩的时间最长的电子竞技之一，已成为人工智能研究的“重大挑战”。

现在，我们介绍我们的星际争霸II计划AlphaStar，这是第一个打败顶级职业球员的人工智能。 在12月19日举行的一系列测试赛中，AlphaStar在与队友Dario “TLO” Wünsch成功进行基准比赛后，以5比0击败了Team Liquid的Grzegorz “MaNa” Komincz，这是世界上最强的专业星际争霸职业选手之一。 比赛是在竞争阶梯地图上的专业比赛条件下进行的，没有任何比赛限制。

尽管在Atari、Mario、Quake III Arena夺旗和Dota 2等视频游戏方面取得了重大成功，但直到现在，AI技术仍在努力应对星际争霸的复杂性。 通过手工制作系统的主要元素，对游戏规则施加重大限制，赋予系统超人能力或通过在简化地图上进行游戏，可以获得最佳结果。 即使进行了这些修改，也没有任何系统可以与专业玩家的技能相媲美。 相比之下，AlphaStar使用深度神经网络进行星际争霸II的完整游戏，该神经网络通过监督学习和强化学习直接从原始游戏数据进行训练。

### 1. 星际争霸的挑战

由暴雪娱乐公司（Blizzard Entertainment）创建的星际争霸II（StarCraft II）设置在一个虚构的科幻世界中，具有丰富的多层次游戏玩法，旨在挑战人类的智慧。 除了最初的冠军头衔之外，它还是有史以来规模最大、最成功的比赛之一，其中有超过20年的职业选手参加电子竞技比赛。

有几种不同的游戏方式，但在电子竞技中，最常见的是参加5场比赛的1v1锦标赛。 首先，玩家必须选择参加三种不同的外星“种族”中的一种：虫族、神族或人族，所有这些都具有鲜明的特征和能力（尽管职业选手倾向于专注于一场比赛）。 每个玩家都从一些工人单位开始，这些单位收集基本资源以建造更多单位和结构并创造新技术。 这反过来又允许玩家收获其他资源，建立更复杂的基础和结构，并开发可用于智胜对手的新功能。 为了获胜，玩家必须谨慎地平衡其经济的宏观管理，以及对其各个单位的低级控制。

平衡短期和长期目标以及适应意外情况的需求对于经常趋于脆弱和不灵活的系统提出了巨大挑战。 掌握这个问题需要在一些AI研究挑战中取得突破，包括：

- 博弈论：星际争霸是一种游戏，就像石头剪刀一样，没有单一的最佳策略。 因此，人工智能训练过程需要不断探索和拓展战略知识的前沿。
- 不完美的信息：与国际象棋或围棋这样的游戏不同，玩家可以看到一切，关键信息是星际争霸玩家隐藏的，必须通过“侦察”积极发现。
- 长期规划：像许多现实世界的问题一样，因果关系不是即时的。 游戏也可能需要长达一个小时才能完成，这意味着游戏早期采取的行动可能无法在很长一段时间内获得回报。
- 实时：与传统棋盘游戏不同，玩家在后续动作之间轮流转换，星际争霸玩家必须随着游戏时间的推移不断执行动作。
- 大型动作空间：必须实时控制数百个不同的单元和建筑物，从而形成可能的组合空间。 除此之外，操作是分层的，可以修改和扩充。 我们对游戏的参数化在每个时间步骤平均有大约10到26个合法行动。

由于这些巨大的挑战，星际争霸已成为人工智能研究的“巨大挑战”。 星际争霸和星际争霸II正在进行的比赛评估了自2009年推出BroodWar API以来的进展，包括AIIDE星际争霸AI比赛、CIG星际争霸比赛、学生星际争霸AI比赛和星际争霸II AI Ladder。 为了帮助社区进一步探索这些问题，我们在2016年和2017年与暴雪合作发布了一套名为PySC2的开源工具，包括迄今为止发布的最大的匿名游戏回放集。 我们现在已经在这项工作的基础上，结合工程和算法突破来设计AlphaStar。

### 2. AlphaStar是如何训练的

AlphaStar还使用了一种新颖的多智能体学习算法。 神经网络最初是由暴雪发布的匿名人类游戏的监督学习训练的。 这使AlphaStar能够通过模仿学习星际争霸阶梯上玩家使用的基本微观和宏观策略。 这个初始代理在95％的比赛中击败了内置的“精英”级AI。

然后将这些用于多智能体强化学习过程的基本。 一个连续的联盟被创建，联盟的代理与竞争者互相玩游戏，类似于人类通过在星际争霸天梯上玩游戏来体验星际争霸的游戏。 通过从现有竞争对手的分支，新的竞争对手被动态地添加到联盟中，然后每个代理从与其他竞争对手的游戏中学习。 这种新形式的训练进一步采用了基于人口和多智能体强化学习的思想，创造了一个不断探索星际争霸游戏玩法的巨大战略空间的过程，同时确保每个竞争者在最强战略上表现良好，并且不会忘记如何打败早先的竞争者。

随着联盟的进步和新的竞争对手的创建，出现了能够击败早期策略的新反策略。 虽然一些新的竞争对手执行的战略仅仅是对先前战略的改进，但其他竞争对手却发现了全新的战略，包括全新的构建订单、单位构成和微观管理计划。 例如，早在AlphaStar联盟中，“俗气”的策略，例如使用Photon Cannons或Dark Templars的快速奔跑，受到青睐。 随着训练的进展，这些风险策略被放弃，导致其他策略：例如，通过过多扩大基地与更多工人获得经济实力，或牺牲两个神谕来破坏对手的工人和经济。 自星际争霸发布以来，这一过程类似于玩家发现新策略的方式，并且能够击败先前偏爱的方法。

为了鼓励联盟中的多样性，每个代理都有自己的学习目标：例如，该代理应该打败哪些竞争对手，以及任何影响代理如何发挥作用的其他内部动机。 一个代理的目标可能是击败一个特定的竞争对手，而另一个代理可能必须击败整个竞争对手的分支，但是通过构建更多的特定游戏单元来实现这一目标。 这些学习目标在训练期间得到调整。

通过对抗竞争对手的游戏，强化学习来更新每个代理的神经网络权重，以优化其个人学习目标。 权重更新规则是一种有效且新颖的离线行为者-评论者强化学习算法，具有经验重放、自我模仿学习和策略提升。

为了训练AlphaStar，我们使用Google的v3 TPU构建了一个高度可扩展的分布式训练设置，支持从数千个星际争霸II的并行实例中学习的代理人群。 AlphaStar联盟运行了14天，每个代理使用16个TPU。 在训练期间，每位代理都经历了长达200年的星际争霸实时游戏。 最终的AlphaStar代理包含联盟的Nash分布的组件 - 换句话说，已经发现的最有效的策略组合 - 在单个桌面GPU上运行。

正在准备对这项工作进行全面的技术描述，以便在同行评审期刊上发表。

### 3. AlphaStar如何运行和观察游戏

TLO和MaNa等专业星际争霸玩家平均每分钟可发出数百个动作（APM）。 这远远少于大多数现有机器人，这些机器人独立控制每个单元并始终如一地维持数千甚至数万个APM。

在针对TLO和MaNa的游戏中，AlphaStar的平均APM约为280，明显低于专业玩家，尽管其行为可能更准确。 这个较低的APM部分是因为AlphaStar使用重放开始训练，因此模仿了人类玩游戏的方式。 此外，AlphaStar平均在观察和行动之间的延迟为350毫秒。

在与TLO和MaNa的比赛中，AlphaStar通过其原始界面直接与星际争霸游戏引擎进行交互，这意味着它可以直接在地图上观察其自身及其对手可见单位的属性，而无需移动相机。 相比之下，人类玩家必须明确管理“注意力经济”，以决定相机的聚焦位置。 然而，对AlphaStar游戏的分析表明它管理着一个隐含的关注焦点。 平均而言，代理每分钟“切换上下文”约30次，类似于MaNa或TLO。

此外，在比赛之后，我们开发了第二版AlphaStar。 与人类玩家一样，此版本的AlphaStar选择何时何地移动相机，其感知仅限于屏幕上的信息，并且动作位置仅限于其可视区域。

我们训练了两个新的代理，一个使用原始界面，一个必须学会控制相机，对抗AlphaStar联盟。 最初通过来自人类数据的监督学习训练每个代理，然后是上面概述的强化学习过程。 使用相机界面的AlphaStar版本几乎与原始界面一样强大，在我们的内部排行榜上超过7000 MMR。 在一场展览比赛中，MaNa使用相机界面击败了AlphaStar的原型版本，该界面仅训练了7天。 我们希望在不久的将来评估完全训练过的相机界面实例。

这些结果表明，AlphaStar对MaNa和TLO的成功实际上是由于卓越的宏观和微观战略决策，而不是优越的点击率、更快的反应时间或原始界面。

### 4. 针对职业选手评估AlphaStar

星际争霸游戏允许玩家选择三种外星种族中的一种：人族、虫族或神族。 我们选择AlphaStar专门参加单人比赛。 请注意，相同的训练路径可以应用于任何种族。 我们的代理接受过训练，可以在CatalystLE阶梯地图上玩Protoss v Protoss游戏中的星际争霸II（v4.6.2）。 为了评估AlphaStar的性能，我们最初测试了我们的代理对抗TLO：顶级专业Zerg玩家和GrandMaster等级Protoss玩家。 AlphaStar以5比0赢得了比赛，使用了各种各样的单位和建立订单。 “我对代理的强大程度感到惊讶，”他说。 “AlphaStar采取了众所周知的战略并将其转变为头脑。 代理演示了我以前没想过的策略，这意味着可能还有新的游戏方式我们还没有完全探索过。“

在对我们的代理进行了为期一周的训练后，我们与世界上最强大的星际争霸II玩家之一MaNa以及10位最强大的Protoss玩家进行了对抗。 AlphaStar再次赢得了5场比赛0，表现出强大的微观和宏观战略技能。 “我看到AlphaStar几乎在每场比赛中都采用了先进的动作和不同的策略，并且使用了一种非常人性化的游戏玩法，这让我印象深刻，”他说。 “我已经意识到我的游戏玩法依赖于强迫错误并能够利用人类的反应，所以这让我对游戏有了全新的启发。 我们都很高兴看到接下来会发生什么。“

### 5. AlphaStar等复杂问题

虽然星际争霸只是一款游戏，虽然它很复杂，但我们认为AlphaStar背后的技术可以用来解决其他问题。例如，它的神经网络架构能够对很长时间的可能行为进行建模，游戏通常持续长达一个小时，成千上万次移动。星际争霸的每一帧都用作输入的一步，神经网络预测每帧之后游戏剩余部分的预期行动顺序。在很长的数据序列上进行复杂预测的基本问题出现在许多现实世界的挑战中，例如天气预报、气候建模、语言理解等等。我们对使用AlphaStar项目的学习和发展在这些领域取得重大进展的潜力感到非常兴奋。

我们还认为我们的一些训练方法可能对安全和强大的AI研究有用。人工智能面临的一大挑战是系统出错的方式，而星际争霸专业人士以前发现，通过找到引发这些错误的创造性方法，很容易击败人工智能系统。 AlphaStar基于联盟的创新训练流程找到了最可靠且最不可能出错的方法。我们对这种方法的潜力感到兴奋，这种方法有助于提高人工智能系统的安全性和稳健性，特别是在能源这样的安全关键领域，这对解决复杂的边缘情况至关重要。

实现最高水平的星际争霸游戏代表了有史以来最复杂的视频游戏之一的重大突破。 我们相信这些进步，以及AlphaZero和AlphaFold等项目最近的其他进展，代表着我们创建智能系统的使命向前迈出了一步，有朝一日，智能系统将帮助我们为世界上一些最重要和最基本的科学问题解开新颖的解决方案。

我们非常感谢Team Liquid的TLO和MaNa的支持和丰富的技能。 我们也非常感谢暴雪和星际争霸社区的持续支持，使这项工作成为可能。







